# -*- coding: utf-8 -*-
"""Recognizing handwritten digits.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uo7mOygSZgU5vRXCtrho7as72zq2aGBk

# **Recognizing handwritten digits with support vector machines**

### **Project Overview**

This project aims to classify handwritten digits (0–9) using cutting-edge deep learning architecture: **Vision Transformers (ViT)**. While earlier models like SVMs, KNNs, CNNs, and Random Forests have shown high accuracy on this task, Vision Transformers offer a modern alternative that leverages self-attention mechanisms, originally introduced in natural language processing, for image classification.

We compare this state-of-the-art method against other classical and deep learning models to observe how attention-based architectures perform in the domain of image recognition.

---

### **Objectives**

- Train a Vision Transformer model on the handwritten digits dataset.
- Evaluate model performance with classification metrics and confusion matrix.
- Compare ViT with traditional ML (SVM, KNN, RF) and deep learning (CNN) approaches.

---

### **Dataset**

We use the **Digits dataset** available in `sklearn.datasets`. It contains 1,797 grayscale images of handwritten digits, each of size **8×8 pixels**, labeled from 0 to 9.

- **Training Samples:** 1,437  
- **Testing Samples:** 360

---

### **Techniques Used**

- **Data Preprocessing:** Rescaling, reshaping, and label encoding  
- **Traditional Models:** SVM, KNN, Random Forest, Logistic Regression  
- **Deep Learning:** CNN using Keras  
- **Advanced Model:** Vision Transformer (ViT) via Hugging Face `transformers` and `datasets`  
- **Evaluation:** Accuracy, Precision, Recall, F1-score, Confusion Matrix

### **Loading the dataset**
"""

# Step 1: Load the Digits Dataset
from sklearn.datasets import load_digits
import pandas as pd
import matplotlib.pyplot as plt

# Load digits dataset
digits = load_digits()

# Check dataset structure
print(f"Shape of images: {digits.images.shape}")  # e.g. (1797, 8, 8)
print(f"Number of labels: {len(digits.target)}")
print("Target classes:", set(digits.target))

# Display first image and its label
plt.gray()
plt.matshow(digits.images[0])
plt.title(f"Label: {digits.target[0]}")
plt.show()

"""### **Preprocessing the Data for SVM**"""

# Step 2: Preprocessing the data for SVM
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Flatten the images (from 8x8 to 64 features)
X = digits.images.reshape((len(digits.images), -1))  # shape: (1797, 64)
y = digits.target

# Normalize pixel intensity
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Check dimensions
print(f"Training samples: {X_train.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

"""### **Support Vector Machine (SVM) Classifier**"""

# Step 3: Train SVM model
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Instantiate the SVM classifier
svm_clf = SVC(kernel='rbf', C=10, gamma=0.01)  # RBF kernel is common for image classification

# Fit the model on the training data
svm_clf.fit(X_train, y_train)

# Predict on the test data
y_pred = svm_clf.predict(X_test)

# Evaluate the performance
print("✅ SVM Model Performance")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""### **K-Nearest Neighbors (KNN) Classifier**"""

# Step 4: Train a K-Nearest Neighbors (KNN) Classifier
from sklearn.neighbors import KNeighborsClassifier

# Instantiate the KNN model
knn_clf = KNeighborsClassifier(n_neighbors=3)

# Fit the model on the training data
knn_clf.fit(X_train, y_train)

# Predict on the test data
y_pred_knn = knn_clf.predict(X_test)

# Evaluate performance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("✅ KNN Model Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn))

"""### **Random Forest Classifier**"""

# Step 5: Train a Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

# Instantiate and train the model
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_clf.predict(X_test)

# Evaluate performance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("✅ Random Forest Model Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

"""### **Logistic Regression Classifier**"""

# Step 6: Train a Logistic Regression Model
from sklearn.linear_model import LogisticRegression

# Instantiate the model with suitable solver
log_reg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial', random_state=42)
log_reg.fit(X_train, y_train)

# Make predictions
y_pred_lr = log_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("✅ Logistic Regression Model Performance")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

"""### **Deep Learning with CNN (Keras + TensorFlow)**"""

# Step 7: Deep Learning with CNN (Keras)
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 7.1 Load and reshape the data
digits = load_digits()
X = digits.images / 16.0  # Normalize pixel values (0 to 1)
y = to_categorical(digits.target)  # One-hot encode labels

# Reshape for CNN: (n_samples, height, width, channels)
X = X.reshape(-1, 8, 8, 1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7.2 Build the CNN model
model = Sequential([
    Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(8, 8, 1)),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(10, activation='softmax')
])

# 7.3 Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 7.4 Train the model
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)

# 7.5 Evaluate the model
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

# 7.6 Metrics
print("✅ CNN Model Performance")
print("Accuracy:", accuracy_score(y_true, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_true, y_pred))
print("\nClassification Report:")
print(classification_report(y_true, y_pred))

"""## **Vision transformer approach**

### **Advanced ViT-based (Vision Transformer)**
"""

# Step 1: Install Required Libraries
!pip install -q transformers datasets evaluate torchvision

# Step 2: Import Libraries
import numpy as np
import torch
from torchvision import transforms
from datasets import load_dataset
from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, DefaultDataCollator
import evaluate

# Step 3: Load MNIST Dataset
dataset = load_dataset("mnist")

# Step 4: Define Processor & Transform
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")
normalize = transforms.Normalize(mean=processor.image_mean, std=processor.image_std)
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    normalize
])

# Step 5: Preprocess function
def preprocess(example):
    example["pixel_values"] = transform(example["image"]).numpy()
    return example

# Step 6: Apply transformation
dataset = dataset.map(preprocess)
dataset = dataset.rename_column("label", "labels")
dataset.set_format(type="torch", columns=["pixel_values", "labels"])

# Step 7: Load Model
model = ViTForImageClassification.from_pretrained("google/vit-base-patch16-224-in21k", num_labels=10)

# Step 8: Metrics
accuracy = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# Step 9: Training Arguments
training_args = TrainingArguments(
    output_dir="./vit-mnist",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="no",
    evaluation_strategy="epoch",
    remove_unused_columns=False
)

# Step 10: Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"].shuffle(seed=42).select(range(10000)),  # Sampled for faster training
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
    tokenizer=processor,
    data_collator=DefaultDataCollator()
)

# Step 11: Train and Evaluate
trainer.train()
trainer.evaluate()

"""# **Conclusion**

This project focuses on recognizing handwritten digits from the classic `digits` dataset using both traditional machine learning models and deep learning techniques. The goal is to compare and contrast their performance and understand which approaches offer the most reliable accuracy for digit classification.

### Dataset
- The project uses the built-in `sklearn.datasets.load_digits` dataset.
- It includes 1,797 grayscale images of size 8×8, each labeled with a digit from 0 to 9.

### Models Implemented

1. Support Vector Machine (SVM)  
   - Achieved highest accuracy among classic models (98.33%)  
   - Delivered precise decision boundaries and low generalization error  

2. K-Nearest Neighbors (KNN)  
   - Simple and effective  
   - Achieved 96.66% accuracy  
   - Performance limited slightly by sensitivity to local variations  

3. Random Forest  
   - Robust ensemble method  
   - Achieved 96.11% accuracy  
   - Slightly behind SVM due to overfitting to local pixel noise  

4. Logistic Regression  
   - Performed surprisingly well with 97.22% accuracy  
   - Linear model capable of generalizing for multiclass classification  

5. Convolutional Neural Network (CNN)  
   - Built using Keras with Conv2D and MaxPooling layers  
   - Achieved 98.61% validation accuracy  
   - Successfully captured spatial hierarchies in pixel structure  

### Performance Insights
- Deep learning (CNN) outperformed all classical models with minimal preprocessing.
- SVM was the top-performing classical ML model, striking a balance between speed and accuracy.
- All models exceeded 96% accuracy, validating the dataset’s quality and task simplicity.
"""