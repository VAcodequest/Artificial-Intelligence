# -*- coding: utf-8 -*-
"""Classification_using_KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y85DnrKYVVo5rQgj7OKuEroW0h5GFcvX

# **Machine Learning Application - Classification using KNN**

## **Project Introduction**
This project applies K-Nearest Neighbors (**KNN**), **Decision Tree**, and **Logistic Regression** algorithms to classify species in the Iris dataset, demonstrating model setup, evaluation, and performance comparison.

### Loading the dataset and splitting the columns as features and labels
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Load the Iris Dataset that contains the information of features and labels
iris = load_iris()
data = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])

# Split the columns into features and labels
X = data.iloc[:, :-1].values
y = data['target'].values

"""### Splitting the dataset into training and testing"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### Standardize the features"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""### Creating and Training the model"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

"""### Predict the labels for the test set"""

y_pred = knn.predict(X_test)

"""### Calculate and print the accuracy"""

accuracy = accuracy_score(y_test, y_pred)
print(f'KNN Accuracy: {accuracy * 100:.2f}%')

"""# **Further analyses**

### Confusion Matrix
"""

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""### Hyperparameter Tuning"""

param_grid = {'n_neighbors': np.arange(1, 31)}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
print(f'Best parameters: {grid_search.best_params_}')

"""### Cross-validation"""

cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), X, y, cv=5)
print(f'Cross-validation scores: {cv_scores}')
print(f'Average cross-validation score: {np.mean(cv_scores):.2f}')

"""### Model Comparison"""

dt = DecisionTreeClassifier()
lr = LogisticRegression(max_iter=200)
classifiers = {'KNN': knn, 'Decision Tree': dt, 'Logistic Regression': lr}
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'{name} Accuracy: {accuracy * 100:.2f}%')

"""### Binarize the output for ROC curve"""

y_bin = label_binarize(y, classes=[0, 1, 2])
n_classes = y_bin.shape[1]

"""### Learn to predict each class against the other"""

classifier = KNeighborsClassifier(n_neighbors=3)
y_score = classifier.fit(X_train, y_train).predict_proba(X_test)
# Binarize the output for ROC curve
y_bin = label_binarize(y, classes=[0, 1, 2])
y_bin_test = label_binarize(y_test, classes=[0, 1, 2])
n_classes = y_bin.shape[1]

"""### Learn to predict each class against the other"""

classifier = KNeighborsClassifier(n_neighbors=3)
y_score = classifier.fit(X_train, y_train).predict_proba(X_test)

"""### Compute ROC curve and ROC area for each class"""

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_bin_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

"""### Plot all ROC curves"""

plt.figure()
colors = ['aqua', 'darkorange', 'cornflowerblue']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""Summary of Model Evaluations on the Iris Dataset:

1. Model Setup:
   - The Iris dataset was split into training (70%) and testing (30%) sets.
   - Feature standardization was applied using StandardScaler.

2. Model Training and Accuracy:
   - Three models were employed: K-Nearest Neighbors (KNN), Decision Tree, and Logistic Regression.
   - All models achieved a perfect 100% accuracy on the test set, indicating exceptional classification performance.

3. Hyperparameter Tuning and Cross-Validation:
   - Hyperparameter tuning via GridSearchCV identified 'n_neighbors': 3 as the optimal setting for KNN.
   - Cross-validation (5 folds) for KNN with the optimal parameters showed a robust average score of 0.97.

4. Confusion Matrix:
   - The confusion matrix for the KNN model displayed no misclassifications across the Iris species (setosa, versicolor, virginica).

5. ROC Curve Analysis:
   - ROC curves for each class demonstrated perfect discriminative ability with an AUC of 1.00 for each species, highlighting the model's effective handling of class distinctions.

This comprehensive evaluation underscores the effectiveness of the models in distinguishing between Iris species, supported by visual and statistical metrics.


"""